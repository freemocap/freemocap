{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a notebook to make a quick and dirty retina centered video from standard Pupil Player export video\n",
    "\n",
    "It'll make one retina centered video from a standard Pupil Payer export video\n",
    "\n",
    "and also a video of the gaze trace\n",
    "\n",
    "I'll put them together in Adobe Premier or, via this script. We'll see lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import signal\n",
    "from scipy.signal import savgol_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getcher paths straight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freemocap session\n",
    "session_id = 'sesh_2022-05-07_17_15_05_pupil_wobble_juggle_0'\n",
    "freemocap_data_folder = Path('C:/Users/jonma/Dropbox/FreeMoCapProject/FreeMocap_Data')\n",
    "session_folder_path = freemocap_data_folder / session_id\n",
    "pupil_data_path = session_folder_path / 'pupil_000'\n",
    "pupil_data_exports_path = pupil_data_path / 'exports' / '000'\n",
    "pupil_export_video_path = pupil_data_exports_path / 'world.mp4'\n",
    "pupil_world_video_path = pupil_data_path / 'world.mp4'\n",
    "\n",
    "# #argp pilot_0\n",
    "# pupil_data_path = Path(r'H:\\Other computers\\My Computer_MocapComputer\\Wirth_ARGP\\ARGP_Main\\data\\2022_04_29\\pupil_data')\n",
    "# pupil_data_exports_path = pupil_data_path / 'exports' / 'Wirth_Pilot_ARGP_2022-04-09_export'\n",
    "# pupil_export_video_path = pupil_data_exports_path / 'argp_pilot_May2022.mp4'\n",
    "\n",
    "\n",
    "pupil_world_video_path = pupil_data_path / 'world.mp4'\n",
    "\n",
    "\n",
    "gaze_positions_path =  pupil_data_exports_path / 'gaze_positions.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open cv video capture object\n",
    "pupil_export_video_cap_object = cv2.VideoCapture(str(pupil_export_video_path))\n",
    "\n",
    "#pupil labs - gaze data\n",
    "pupil_dataframe = pd.read_csv(gaze_positions_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get some info from the video cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_width : 1280\n",
      "video_height: 720\n",
      "video_framerate: 59\n"
     ]
    }
   ],
   "source": [
    "video_width = int(pupil_export_video_cap_object.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height =  int(pupil_export_video_cap_object.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "video_framerate = int(pupil_export_video_cap_object.get(cv2.CAP_PROP_FPS))\n",
    "print(f'video_width : {video_width }')\n",
    "print(f'video_height: {video_height}')\n",
    "print(f'video_framerate: {video_framerate}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gaze locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_pos_x  = pupil_dataframe.norm_pos_x\n",
    "norm_pos_y = pupil_dataframe.norm_pos_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.Figure(123)\n",
    "# fig.plot(norm_pos_x, label = 'norm_pos_x')\n",
    "# fig.plot(norm_pos_y, label ='norm_pos_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gaze_on_screen_x = norm_pos_x * video_width\n",
    "gaze_on_screen_y = norm_pos_y * video_height\n",
    "\n",
    "world_camera_frame_index_all = pupil_dataframe.world_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero-lag 4th order butterworth filter with a 7Hz cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nans in gaze_on_screen_x: 0\n",
      "nans in gaze_on_screen_y: 0\n",
      "nans in gaze_on_screen_x_filtered: 0\n",
      "nans in gaze_on_screen_y_filtered: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "order = 5\n",
    "cutoff=10**-1\n",
    "b, a = signal.butter(order, cutoff)\n",
    "gaze_on_screen_x_filtered = signal.filtfilt(b, a, gaze_on_screen_x)\n",
    "gaze_on_screen_y_filtered = signal.filtfilt(b, a, gaze_on_screen_y)\n",
    "\n",
    "print(f\"nans in gaze_on_screen_x: {np.isnan(gaze_on_screen_x).sum()}\")\n",
    "print(f\"nans in gaze_on_screen_y: {np.isnan(gaze_on_screen_y).sum()}\")\n",
    "print(f\"nans in gaze_on_screen_x_filtered: {np.isnan(gaze_on_screen_x_filtered).sum()}\")\n",
    "print(f\"nans in gaze_on_screen_y_filtered: {np.isnan(gaze_on_screen_y_filtered).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([604.55319855, 604.96959393, 605.31206249, ..., 475.75659332,\n",
       "       475.6162116 , 475.47394065])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaze_on_screen_x_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b6409dab88>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "plt.cla()\n",
    "fig2 = plt.Figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(gaze_on_screen_x, label = 'gaze_on_screen_x_raw')\n",
    "ax.plot(gaze_on_screen_y, label = 'gaze_on_screen_y_raw')\n",
    "ax.plot(gaze_on_screen_x_filtered, label = 'gaze_on_screen_x_filtered')\n",
    "ax.plot(gaze_on_screen_y_filtered, label = 'gaze_on_screen_y_filtered')\n",
    "ax.set_ylim((0-100, video_width+100))\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "squash data down so each frame is the average of all data recorded on that frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonma\\miniconda3\\envs\\freemocap-env\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1111: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "number_of_frames = np.max(world_camera_frame_index_all)\n",
    "number_of_frames\n",
    "\n",
    "gaze_on_screen_x_downsampled = []\n",
    "gaze_on_screen_y_downsampled = []\n",
    "\n",
    "for this_world_frame_index in range(number_of_frames):\n",
    "    this_frame_data = world_camera_frame_index_all == this_world_frame_index\n",
    "    this_frame_x_data = gaze_on_screen_x_filtered[this_frame_data]\n",
    "    this_frame_y_data = gaze_on_screen_y_filtered[this_frame_data]\n",
    "    gaze_on_screen_x_downsampled.append(np.nanmedian(this_frame_x_data))\n",
    "    gaze_on_screen_y_downsampled.append(np.nanmedian(this_frame_y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"nans in gaze_on_screen_x: {np.isnan(gaze_on_screen_x).sum()}\")\n",
    "print(f\"nans in gaze_on_screen_y: {np.isnan(gaze_on_screen_y).sum()}\")\n",
    "print(f\"nans in gaze_on_screen_x_filtered: {np.isnan(gaze_on_screen_x_filtered).sum()}\")\n",
    "print(f\"nans in gaze_on_screen_y_filtered: {np.isnan(gaze_on_screen_y_filtered).sum()}\")\n",
    "print(f\"nans in gaze_on_screen_x_downsampled: {np.isnan(gaze_on_screen_x_downsampled).sum()}\")\n",
    "print(f\"nans in gaze_on_screen_y_downsampled: {np.isnan(gaze_on_screen_y_downsampled).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# order = 4\n",
    "# cutoff=10**-1\n",
    "# b, a = signal.butter(order, cutoff)\n",
    "# gaze_on_screen_x_filtered = signal.filtfilt(b, a, gaze_on_screen_x_downsampled)\n",
    "# gaze_on_screen_y_filtered = signal.filtfilt(b, a, gaze_on_screen_y_downsampled)\n",
    "\n",
    "print(f\"nans in gaze_on_screen_x: {np.isnan(gaze_on_screen_x).sum()}\")\n",
    "print(f\"nans in gaze_on_screen_y: {np.isnan(gaze_on_screen_y).sum()}\")\n",
    "print(f\"nans in gaze_on_screen_x_filtered: {np.isnan(gaze_on_screen_x_filtered).sum()}\")\n",
    "print(f\"nans in gaze_on_screen_y_filtered: {np.isnan(gaze_on_screen_y_filtered).sum()}\")\n",
    "\n",
    "# plt.close('all')\n",
    "# fig2 = plt.Figure()\n",
    "# ax = plt.gca()\n",
    "# # ax.plot(gaze_on_screen_x_downsampled, label = 'gaze_on_screen_x_downsampled')\n",
    "# # ax.plot(gaze_on_screen_y_downsampled, label = 'gaze_on_screen_y_downsampled')\n",
    "# ax.plot(gaze_on_screen_x_filtered, label = 'gaze_on_screen_x_filtered')\n",
    "# ax.plot(gaze_on_screen_y_filtered, label = 'gaze_on_screen_y_filtered')\n",
    "# ax.set_ylim((0-100, video_width+100))\n",
    "# ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#smoooooooooth, just a bit\n",
    "smoothWinLength = 5\n",
    "smoothOrder = 3\n",
    "\n",
    "gaze_on_screen_x_downsampled_smoothed = savgol_filter(gaze_on_screen_x_downsampled, smoothWinLength, smoothOrder)\n",
    "gaze_on_screen_y_downsampled_smoothed = savgol_filter(gaze_on_screen_y_downsampled, smoothWinLength, smoothOrder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig2 = plt.Figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(gaze_on_screen_x_downsampled, '.-', label = 'gaze_on_screen_x_downsampled')\n",
    "ax.plot(gaze_on_screen_y_downsampled, '.-',label = 'gaze_on_screen_y_downsampled')\n",
    "ax.plot(gaze_on_screen_x_downsampled_smoothed, '.-', label = 'gaze_on_screen_x_downsampled_smoothed')\n",
    "ax.plot(gaze_on_screen_y_downsampled_smoothed, '.-',label = 'gaze_on_screen_y_downsampled_smoothed')\n",
    "\n",
    "ax.set_ylim((0-100, video_width+100))\n",
    "ax.legend(loc='upper left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"nans in gaze_on_screen_x_downsampled: {np.isnan(gaze_on_screen_x_downsampled).sum()}\")\n",
    "print(f\"nans in gaze_on_screen_y_downsampled: {np.isnan(gaze_on_screen_y_downsampled).sum()}\")\n",
    "\n",
    "gaze_x_df = pd.DataFrame(gaze_on_screen_x_downsampled)\n",
    "gaze_x_df.interpolate(method = 'linear', inplace = True)\n",
    "gaze_on_screen_x_downsampled_gapfilled =  gaze_x_df.to_numpy()\n",
    "\n",
    "gaze_y_df = pd.DataFrame(gaze_on_screen_y_downsampled)\n",
    "gaze_y_df.interpolate(method = 'linear', inplace = True)\n",
    "gaze_on_screen_y_downsampled_gapfilled =  gaze_y_df.to_numpy()\n",
    "\n",
    "print(f\"nans in gaze_on_screen_x_downsampled_gapfilled: {np.isnan(gaze_on_screen_x_downsampled_gapfilled).sum()}\")\n",
    "print(f\"nans in gaze_on_screen_y_downsampled_gapfilled: {np.isnan(gaze_on_screen_y_downsampled_gapfilled).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "big_blank_image = np.ones((int(video_height)*2, int(video_width)*2, 3), dtype=np.uint8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pupil_world_video_cap_object = cv2.VideoCapture(str(pupil_world_video_path))\n",
    "\n",
    "# frame_num = 0\n",
    "# success = True\n",
    "# while success:    \n",
    "#     success, image_raw = pupil_world_video_cap_object.read()\n",
    "#     if not success:\n",
    "#         break\n",
    "#     frame_num += 1\n",
    "# print(f\"frame_num: {frame_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create circular mask\n",
    "\n",
    "from =- https://stackoverflow.com/questions/61516526/how-to-use-opencv-to-crop-circular-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define circles\n",
    "radius = video_height #pixels\n",
    "x_center = video_width\n",
    "y_center = video_height\n",
    "\n",
    "# draw filled circles in white on black background as masks\n",
    "circular_mask = np.zeros_like(big_blank_image)\n",
    "circular_mask = cv2.circle(circular_mask, (x_center,y_center), radius, (255,255,255), -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_framerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pupil_world_video_cap_object = cv2.VideoCapture(str(pupil_world_video_path))\n",
    "\n",
    "this_retinal_aligned_image = big_blank_image.copy()\n",
    "this_retinal_aligned_image[:, video_width-video_height:(video_width-video_height)+2*video_height,:]\n",
    "\n",
    "retinal_video_save_path = pupil_data_exports_path / 'retinal_aligned_video.mp4'\n",
    "retinal_video_writer = cv2.VideoWriter(str(retinal_video_save_path),\n",
    "                                       cv2.VideoWriter_fourcc(*\"MP4V\"),\n",
    "                                       video_framerate, \n",
    "                                       (this_retinal_aligned_image.shape[0], this_retinal_aligned_image.shape[0]))\n",
    "\n",
    "success = True\n",
    "for frame_number in range(gaze_on_screen_x_downsampled_gapfilled.shape[0]):\n",
    "    success, image= pupil_world_video_cap_object.read()\n",
    "    if not success:\n",
    "        print('failed to read frame')\n",
    "        break\n",
    "\n",
    "    # image  = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    this_retinal_aligned_image = big_blank_image.copy()\n",
    "\n",
    "    #pull out gaze data to align image to retinal coordinates\n",
    "    gx = int(gaze_on_screen_x_downsampled_gapfilled[frame_number])\n",
    "    gy = int(gaze_on_screen_y_downsampled_gapfilled[frame_number])\n",
    "\n",
    "    bottom_edge_row = int(video_height)*2-(int(video_height)-gy)\n",
    "    top_edge_row = int(video_height) -(int(video_height)-gy)\n",
    "    left_col = (int(video_width)-gx)\n",
    "    right_col = int(video_width)+(int(video_width)-gx)\n",
    "\n",
    "    cols =  np.arange(top_edge_row,bottom_edge_row)\n",
    "\n",
    "    #if things are going off screen, use whatever gaze data worked most recently\n",
    "    if top_edge_row < 0:\n",
    "        gy = previous_gy\n",
    "        top_edge_row = int(video_height) -(int(video_height)-gy)\n",
    "        bottom_edge_row = int(video_height)*2-(int(video_height)-gy)\n",
    "\n",
    "    if bottom_edge_row > int(video_height)*2:\n",
    "        gy = previous_gy\n",
    "        top_edge_row = int(video_height) -(int(video_height)-gy)\n",
    "        bottom_edge_row = int(video_height)*2-(int(video_height)-gy)\n",
    "    \n",
    "    if left_col < 0:\n",
    "        gx = previous_gx\n",
    "        right_col = int(video_width)+(int(video_width)-gx)\n",
    "        left_col = (int(video_width)-gx)\n",
    "    \n",
    "    if right_col > int(video_width)*2:\n",
    "        gx = previous_gx\n",
    "        right_col = int(video_width)+(int(video_width)-gx)\n",
    "        left_col = (int(video_width)-gx)\n",
    "\n",
    "    previous_gx = gx    \n",
    "    previous_gy = gy\n",
    "    \n",
    "    this_retinal_aligned_image[top_edge_row:bottom_edge_row, left_col:right_col,:] = image\n",
    "\n",
    "    this_retinal_aligned_image[circular_mask==0] = 0\n",
    "\n",
    "\n",
    "    this_retinal_aligned_image = this_retinal_aligned_image[:, video_width-video_height:(video_width-video_height)+2*video_height,:]\n",
    "\n",
    "    this_retinal_aligned_image = cv2.line(this_retinal_aligned_image, \n",
    "                                            (int(video_height), 0), \n",
    "                                            (int(video_height), int(video_height)*2), \n",
    "                                            (0,0,0,255), \n",
    "                                            5)\n",
    "    this_retinal_aligned_image = cv2.line(this_retinal_aligned_image, \n",
    "                                            (0, int(video_height)), \n",
    "                                            (int(video_width)*2, \n",
    "                                            int(video_height)), \n",
    "                                            (0,0,0,255), \n",
    "                                            5)\n",
    "    this_retinal_aligned_image = cv2.line(this_retinal_aligned_image, \n",
    "                                            (int(video_height), 0), \n",
    "                                            (int(video_height), int(video_height)*2), \n",
    "                                            (255,255,255,255), \n",
    "                                            1,)\n",
    "    this_retinal_aligned_image = cv2.line(this_retinal_aligned_image, \n",
    "                                            (0, int(video_height)), \n",
    "                                            (int(video_width)*2, \n",
    "                                            int(video_height)), \n",
    "                                            (255,255,255,255), \n",
    "                                            1)\n",
    "    this_retinal_aligned_image = cv2.circle(this_retinal_aligned_image, \n",
    "                                            (int(video_height), int(video_height)),\n",
    "                                            20,\n",
    "                                            (0,0,0,255), \n",
    "                                            5)\n",
    "    this_retinal_aligned_image = cv2.circle(this_retinal_aligned_image, \n",
    "                                            (int(video_height), int(video_height)),\n",
    "                                            20,\n",
    "                                            (255,255,255,255), \n",
    "                                            1)\n",
    "\n",
    "    # plt.cla()\n",
    "    # plt.imshow(this_retinal_aligned_image[:,:,:3]) \n",
    "    # plt.show()\n",
    "    # plt.pause(0.1)\n",
    "\n",
    "    retinal_video_writer.write(this_retinal_aligned_image) \n",
    "\n",
    "    if frame_number%100 == 0:\n",
    "        print(f'frame_number: {frame_number}')\n",
    "\n",
    "retinal_video_writer.release()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "122c9931a57dd91c579842728ee2ee1c7ffefd770186c077f442b22e7e57f48e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('freemocap-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
